# ğŸ“„ DocQuery â€“ Modular PDF RAG Assistant

DocQuery is a **modular Retrieval-Augmented Generation (RAG) system** that enables users to ask natural-language questions over a collection of PDF documents.
The system retrieves semantically relevant document chunks using embeddings and a vector database, then generates **grounded, source-aware answers** using an LLM.

This project was built to **understand and implement real-world RAG architecture**, focusing on correctness, modularity, safety, and evaluation rather than just producing answers.

---

## âœ¨ Key Features

* ğŸ“š **Multi-PDF ingestion** with automatic document parsing
* âœ‚ï¸ **Configurable text chunking** with overlap for semantic continuity
* ğŸ§  **SentenceTransformer embeddings** for semantic search
* ğŸ—‚ï¸ **ChromaDB vector store** with persistence and metadata
* ğŸ” **Top-K similarity-based retrieval**
* ğŸ¤– **LLM-powered answer generation (Groq)**
* ğŸ“Œ **Source attribution with similarity scores**
* ğŸ§ª **Graceful handling of out-of-scope questions**
* ğŸ›‘ **Safety-aware responses** (medical/unsupported queries are refused)
* ğŸ–¥ï¸ **Interactive Streamlit UI** with PDF selection

---

ğŸ“Œ **Flow overview**


![alt text](<screenshots/flow diagram.png>)


## ğŸ–¥ï¸ User Interface

The Streamlit UI allows users to:

* View available PDFs
* Select which document(s) to query
* Ask natural-language questions
* See generated answers
* Inspect **source documents with similarity scores**

ğŸ“¸ **UI Screenshots**

![alt text](<screenshots/demo query 3.png>) 
![alt text](<screenshots/demo query 2.png>)
 ![alt text](<screenshots/demo query.png>) 
 ![alt text](<screenshots/testing safety .png>)

## ğŸ§ª Evaluation & Observations

### âœ”ï¸ In-Scope Questions

* Factual and design-related questions retrieve relevant chunks
* Similarity scores typically range from **0.5â€“0.8**
* Higher scores are observed for focused, single-intent queries

### âŒ Out-of-Scope Questions

Examples:

* General knowledge (sports, cooking, math)
* Made-up concepts
* Content not present in PDFs

**Behavior:**

* System responds with *â€œNo relevant information found in the provided documentsâ€*
* Prevents hallucinated answers

### ğŸ›‘ Safety Handling

When asked **medical or unsupported advisory questions**, the LLM:

* Refuses to answer
* Avoids generating unsafe or misleading content

This aligns with **responsible AI behavior**.

---

## ğŸ“Š Similarity Score Interpretation

Similarity scores reflect **retrieval confidence**, not answer quality.

| Score Range | Interpretation            |
| ----------- | ------------------------- |
| 0.75 â€“ 1.0  | Strong semantic match     |
| 0.6 â€“ 0.75  | Good contextual relevance |
| 0.45 â€“ 0.6  | Partial relevance         |
| < 0.45      | Likely unrelated          |

Multi-part questions often yield lower similarity scores due to **embedding dilution**, even when the final answer is correct.

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **SentenceTransformers**
* **ChromaDB**
* **Groq LLM**
* **Streamlit**
* **LangChain-style RAG concepts (custom implementation)**

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Add PDFs

Place your PDF files in:

```
data/pdfs/
```

### 3ï¸âƒ£ Run the app

```bash
streamlit run app.py
```

---

## ğŸ“Œ Future Improvements

* Query decomposition for multi-intent questions
* Hybrid search (BM25 + embeddings)
* Inline citations inside answers
* Conversation memory
* REST API backend (FastAPI)




